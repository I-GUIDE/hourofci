{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Parallel Computing\n",
    "\n",
    "### Segment 5 of 6\n",
    "\n",
    "### PySpark SQL III: Spatial is Special!\n",
    "\n",
    "#### In this segment we will learn:\n",
    "* Apache Sedona\n",
    "* Querying spatial data with PySpark SQL.\n",
    "\n",
    "\n",
    "*Lesson Developer: Mohsen Ahmadkhani, ahmad178@umn.edu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder\n",
    "<a href=\"#/slide-2-0\" class=\"navigate-right\" style=\"background-color:blue;color:white;padding:8px;margin:2px;font-weight:bold;\">Continue with the lesson</a>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "<font size=\"+1\">\n",
    "\n",
    "By continuing with this lesson you are granting your permission to take part in this research study for the Hour of Cyberinfrastructure: Developing Cyber Literacy for GIScience project. In this study, you will be learning about cyberinfrastructure and related concepts using a web-based platform that will take approximately one hour per lesson. Participation in this study is voluntary.\n",
    "\n",
    "Participants in this research must be 18 years or older. If you are under the age of 18 then please exit this webpage or navigate to another website such as the Hour of Code at https://hourofcode.com, which is designed for K-12 students.\n",
    "\n",
    "If you are not interested in participating please exit the browser or navigate to this website: http://www.umn.edu. Your participation is voluntary and you are free to stop the lesson at any time.\n",
    "\n",
    "For the full description please navigate to this website: <a href=\"../../gateway-lesson/gateway/gateway-1.ipynb\">Gateway Lesson Research Study Permission</a>.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide"
    ]
   },
   "outputs": [],
   "source": [
    "# This code cell starts the necessary setup for Hour of CI lesson notebooks.\n",
    "# First, it enables users to hide and unhide code by producing a 'Toggle raw code' button below.\n",
    "# Second, it imports the hourofci package, which is necessary for lessons and interactive Jupyter Widgets.\n",
    "# Third, it helps hide/control other aspects of Jupyter Notebooks to improve the user experience\n",
    "# This is an initialization cell\n",
    "# It is not displayed because the Slide Type is 'Skip'\n",
    "\n",
    "from IPython.display import HTML, IFrame, Javascript, display, Latex\n",
    "from ipywidgets import interactive, Layout\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import getpass # This library allows us to get the username (User agent string)\n",
    "\n",
    "# import package for hourofci project\n",
    "import sys\n",
    "sys.path.append('../../supplementary') # relative path (may change depending on the location of the lesson notebook)\n",
    "import hourofci\n",
    "\n",
    "# load javascript to initialize/hide cells, get user agent string, and hide output indicator\n",
    "# hide code by introducing a toggle button \"Toggle raw code\"\n",
    "HTML(''' \n",
    "    <script type=\"text/javascript\" src=\\\"../../supplementary/js/custom.js\\\"></script>\n",
    "    \n",
    "    <style>\n",
    "        .output_prompt{opacity:0;}\n",
    "    </style>\n",
    "    \n",
    "    <input id=\"toggle_code\" type=\"button\" value=\"Toggle raw code\">\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Distributed Spatial Computing?\n",
    "\n",
    "<center><img src=https://media.makeameme.org/created/why-even-bother-5c8eb2.jpg width=300></center>\n",
    "In recent years, the spatial technology has evolved tremendously resulted in BIG spatial data. Some examples of spatial big data include location-based services like Uber, Lyft, scooter ride companies and many more, remote sensing data, spatial social networks' data like twitter and FaceBook, weather maps, transportation, and countless others. Handling such BIG load of spatial data needs <b>faster</b> database management technologies, and parallel computing is faster!\n",
    "\n",
    "By the way, if you are curious about spatial big data, we talked about it in the <a href=\"http://try.hourofci.org/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fhourofci%2Flessons&urlpath=tree%2Flessons%2Fintermediate-lessons%2Fbig-data%2FWelcome.ipynb&branch=master\">intermediate Big Data</a> lesson. So, take a look if you have not already.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Sedona\n",
    "\n",
    "So far, we briefly saw how Apache Spark works but like most data management technologies, Apache Spark also first was developed for non-spatial data. Why? well, because <b>spatial is special!</b> Due to nature of spatial data type, it is much more complex and therefore harder to store and analyse. \n",
    "To support spatial data type, people at Apache launched an extension to Spark named <b><a href=\"http://sedona.apache.org\">Apache Sedona</a></b>. \n",
    "\n",
    "Apache Sedona (formerly GeoSpark) is a powerful tool that extends RDDs to geospatial RDDs (aka SpatialRDD). In simple words Apache Sedona enables two major things: \n",
    "<ol>\n",
    "    <li>\n",
    "Distributing geospatial data between multiple computational cores \n",
    "    </li>\n",
    "    <li>\n",
    "Spatial functions and queries in SparkSQL\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "In this segment we touch on Apache Sedona and see how it works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import `SparkSession` from SparkSQL as the initial step in Spark framework. Then we need to import a few sub-modules from Sedona. But we need to install the `Apache Sedona` package first. Let's do it in the next cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install apache-sedona --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, click the \"Restart Kernel\" to update the list of installed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "def restarter():\n",
    "    display(HTML(\n",
    "        '''\n",
    "            <script>\n",
    "                code_show = false;\n",
    "                function restart_kernel(){\n",
    "                    IPython.notebook.kernel.restart();\n",
    "                }\n",
    "            </script>\n",
    "            <button onclick=\"restart_kernel()\">Restart Kernel</button>\n",
    "        '''\n",
    "    ))\n",
    "restarter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to import the packages for this walkthrough including sedona, pyspark, geopandas (for data processing) and ipyleaflet (for mapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "import geopandas as gpd\n",
    "from ipyleaflet import Map, GeoData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a spatially enabled spark context using the imported Sedona sub-modules. Don't worry too much if it looks complicated! You can copy and paste this for your project :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"Spatial Spark Demo\").\\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) .\\\n",
    "    config(\"spark.jars.packages\", \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.2.1-incubating,org.datasyslab:geotools-wrapper:1.1.0-25.2\") .\\\n",
    "    getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline \n",
    "To illustrate how faster Spark SQL can work through parallelization, we will compare it's performance with GeoPandas module. In this segment, we will execute a spatial operation between two sample spatial datasets. Specifically, we want to know:\n",
    ">**Which rivers and lake centerlines intersect the states of Minnesota and Washington?**. \n",
    " \n",
    "\n",
    "To find out the answer, we will take the following steps: \n",
    "><ol>\n",
    "    <li>\n",
    "        Downloading and reading the <i>US states'</i> and world's <i>rivers</i> datasets as <i>GeoPandas</i> dataframes. \n",
    "    </li>\n",
    "    <li>\n",
    "        Perform the spatial <b>intersection</b> operation using <b>GeoPandas</b> and record the operation time. \n",
    "    </li>\n",
    "    <li>\n",
    "        Do the same thing using spatial <b>Spark SQL (Apache Sedona)</b> and record the operation time. \n",
    "    </li>\n",
    "    <li>\n",
    "        Comparing the execution times and concluding.\n",
    "    </li>\n",
    "</ol>\n",
    "    \n",
    "Let's do it! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Downloading demo shapefiles to illustrate Apache Sedona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World-Scale Rivers and Lake Centerlines Dataset\n",
    "This shapefile includes all the rivers and lake centerlines in the world with 10 meters accuracy. \n",
    "This is the same dataset we used <a href=\"http://try.hourofci.org/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fhourofci%2Flessons&urlpath=tree%2Flessons%2Fbeginner-lessons%2Fgeospatial-data%2Fgd-example_1.ipynb&branch=master\">here</a> in the beginner geospatial data lesson. You can learn more about this dataset there. \n",
    "\n",
    "Run the cell below to download the dataset as a zip file using the `wget` module and then extract the shapefile using `unzip`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O ne_10m_rivers_lake_centerlines.zip https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/physical/ne_10m_rivers_lake_centerlines.zip \n",
    "!unzip -n ne_10m_rivers_lake_centerlines.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the shapefile using geopandas and take a look at the first few rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use geopandas to read the file we just downloaded and unzipped through wget and unzip \n",
    "rivers = gpd.read_file('ne_10m_rivers_lake_centerlines.shp')\n",
    "# Look at the first few rows\n",
    "rivers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US States Dataset\n",
    "\n",
    "Next, we will do the same for the `US states` shapefile. The `US states` shapefile contains the polygons of the US states. Run the following two cells to download it from the US Census government's <a href=\"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjBpayMwJn7AhV2lGoFHbfWB_cQFnoECAsQAQ&url=https%3A%2F%2Fwww.census.gov%2Fgeographies%2Fmapping-files%2Ftime-series%2Fgeo%2Fcarto-boundary-file.html&usg=AOvVaw2QKo7f-rChpkoO7zQ9E75A\">data warehouse</a> and read it as a GeoPandas Dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O us_states.zip https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_20m.zip \n",
    "!unzip -n us_states.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = gpd.read_file('us_states.zip')\n",
    "states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Impotrant Caveat\n",
    "\n",
    "Before we start working with multiple spatial datasets we MUST make sure they are all in the same <a href='https://en.wikipedia.org/wiki/Spatial_reference_system'>coordinate reference system (CRS)</a>.\n",
    "\n",
    "We can pull the coordinate system information of each dataframe using the `.crs` attribute. Run the following two cells to see the coordinate systems information for our two dataframes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, they are using different coordinate systems (WGS84 for rivers vs. NAD83 for states). We can use GeoPandas' `.to_crs(\"CRS ID\")` method to change the the CRS of a dataframe. \n",
    "\n",
    "In the next cell, we change the CRS of `states` dataframe to WGS84 by passing it's CRS ID (4326) as an argument so both dataframes are at the same coordinate system. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = states.to_crs('4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping and Visualization\n",
    "\n",
    "To visualize the loaded spatial datasets we use ipyleaflet package. In the next two cells we create a data *layer*. Then we make a map using `Map` module setting a center point and an appropriate zoom level. These arguments vary according to the geographical extent of the datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ipyleaflet layer for the rivers' dataset using GeoData module\n",
    "rivers_layer = GeoData(geo_dataframe = rivers, style={'color':'blue'})\n",
    "# create a map object and specify appropriate center point and zoom level according the extent of the data\n",
    "mymap1 = Map(center=(40,10), zoom = 2)\n",
    "# add the rivers' layer to the map to visualize\n",
    "mymap1.add_layer(rivers_layer)\n",
    "mymap1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "states_layer = GeoData(geo_dataframe = states, style={'color':'red'})\n",
    "mymap2 = Map(center=(40,-100), zoom = 4)\n",
    "mymap2.add_layer(states_layer)\n",
    "mymap2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform the spatial intersection operation using *GeoPandas* and record the operation time. \n",
    "\n",
    "Now that we have a better sense of our datasets by visualizing, it's time to run the intersection operation using **GeoPandas**. To do this, we will first filter the two states of Minnesota and Washington using the following code.\n",
    "\n",
    "Note that here the character `|` is the logical `OR` operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states[(states['NAME']=='Minnesota')|(states['NAME']=='Washington')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To operate the spatial function of `intersection` we will use GeoPandas `overlay` function setting its `how=` argument to `intersection`. \n",
    "\n",
    "We record the execution time using the `time` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "inter = gpd.overlay(rivers, states[(states['NAME']=='Minnesota')|(states['NAME']=='Washington')], how='intersection')\n",
    "\n",
    "gpd_time = time.time() - start_time\n",
    "print(f\"Execution time for GeoPandas: {gpd_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform the spatial intersection operation using *Apache Sedona* and record the operation time. \n",
    "\n",
    "\n",
    "### Converting GeoPandas to Apache Sedona\n",
    "\n",
    "The first step in using Apache Sedona is to convert our input data to `spark dataframes`, something that Spark understands and is able to parallelize. We can convert our GeoPandas dataframes to spark dataframes using Spark's `createDataFrame` method as below. \n",
    "\n",
    "Here we also print the dataframe's schema to see what columns and data types we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_spdf = spark.createDataFrame(states)\n",
    "states_spdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that at the very buttom of the schema there is a `geometry` data type! That's what Sedona brought to us. \n",
    "\n",
    "Next we use `show` method to see a few first rows of the spark dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "states_spdf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And same process for the rivers' dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers_spdf = spark.createDataFrame(rivers)\n",
    "rivers_spdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SQL Views (Virtual Tables)\n",
    "\n",
    "Similar to non-spatial data, we need to create SQL Views for each of the dataframes we have. This is a required step to enable querying data in SQL and literally means creating virtual relations (tables) for dataframes. Below, we do this using `createOrReplaceTempView` method and create two Views named `rivers_table` and `states_table`. We will query from these two tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers_spdf.createOrReplaceTempView(\"rivers_table\")\n",
    "states_spdf.createOrReplaceTempView(\"states_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection Spatial Query \n",
    "\n",
    "Apache Sedona uses SQL language so we need to write a spatial query.<br>\n",
    "In the following SQL query, we select state and river names along with the rivers' geometry. \n",
    "\n",
    "Under the `WHERE` clause, we set the conditions of *the state name being either Minnesota OR Washington* AND *the rivers intersect the polygons of these two states*. \n",
    "\n",
    "> ```sql\n",
    ">SELECT states_table.NAME, rivers_table.name as river_name, rivers_table.geometry geom\n",
    ">FROM states_table, rivers_table\n",
    ">WHERE states_table.NAME IN ('Minnesota', 'Washington') and ST_INTERSECTS(rivers_table.geometry, states_table.geometry)\n",
    "\n",
    "\n",
    "\n",
    "Let's execute this spatial query together and record its execution time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "mn_rivers = spark.sql(\"\"\"\n",
    "SELECT states_table.NAME, rivers_table.name as river_name, rivers_table.geometry as geom\n",
    "FROM states_table, rivers_table\n",
    "WHERE states_table.NAME IN ('Minnesota', 'Washington') and ST_INTERSECTS(rivers_table.geometry, states_table.geometry)\n",
    "\"\"\")\n",
    "\n",
    "sedona_time = time.time() - start_time\n",
    "print(f\"Execution time for Sedona: {sedona_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing the Execution Times and Concluding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if round(gpd_time/sedona_time, 2)>0:\n",
    "    display(Latex(f'''\n",
    "    Comparing the two execution times, for executing the intersection operation GeoPandas took {round(gpd_time, 2)} seconds and \n",
    "    Apache Sedona did it for us in {round(sedona_time, 2)} seconds. It means parallelizing this operation with Spark SQL made \n",
    "    the spatial operation {round(gpd_time/sedona_time, 2)} times FASTER. \n",
    "    '''))\n",
    "else:\n",
    "    display(Latex(f'''\n",
    "    Comparing the two execution times, for executing the intersection operation GeoPandas took {round(gpd_time, 2)} seconds and \n",
    "    Apache Sedona did it for us in {round(sedona_time, 2)} seconds. Although we parallelized the operation using Spark framework, \n",
    "    it did slightly slower than GeoPandas. This is because of the trade-off between the partitioning cost and the load of the input data. \n",
    "    '''))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Spatial queries like this one would take much longer if you do not partition them. Please note that this difference is more significant for larger datasets as the \"distribution\" of data between multiple cores itself could be time consuming. Hence, for small datasets we do not usually use parallel computing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Converting Spark Dataframes Back to GeoPandas For Visualization\n",
    "\n",
    "Now to spatially visualize our query result we need to convert the result back to geopandas dataframe. To do this we simply use `toPandas` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_rivers_df = mn_rivers.toPandas()\n",
    "result = gpd.GeoDataFrame(mn_rivers_df, geometry=\"geom\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the result on a map! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_mn = states[(states['NAME']=='Minnesota') | (states['NAME']=='Washington')]\n",
    "wa_mn_layer = GeoData(geo_dataframe = wa_mn, style={'color':'red'})\n",
    "gdf_layer = GeoData(geo_dataframe = result, style={'color':'blue'})\n",
    "gdf_map = Map(center=(40,-100), zoom = 4)\n",
    "gdf_map.add_layer(gdf_layer)\n",
    "gdf_map.add_layer(wa_mn_layer)\n",
    "gdf_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now, click the link below to go to the exploration segment to dig in even more! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\"><a style=\"background-color:blue;color:white;padding:12px;margin:10px;font-weight:bold;\" \n",
    "href=\"pc-exploration.ipynb\">Click here to go to the next notebook.</a></font>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
