{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Parallel Computing\n",
    "\n",
    "### Segment 5 of 6\n",
    "\n",
    "### PySpark SQL III: Spatial is Special!\n",
    "\n",
    "#### In this segment we will learn:\n",
    "* Apache Sedona\n",
    "* Querying spatial data with PySpark SQL.\n",
    "\n",
    "\n",
    "*Lesson Developer: Mohsen Ahmadkhani, ahmad178@umn.edu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder\n",
    "<a href=\"#/slide-2-0\" class=\"navigate-right\" style=\"background-color:blue;color:white;padding:8px;margin:2px;font-weight:bold;\">Continue with the lesson</a>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "<font size=\"+1\">\n",
    "\n",
    "By continuing with this lesson you are granting your permission to take part in this research study for the Hour of Cyberinfrastructure: Developing Cyber Literacy for GIScience project. In this study, you will be learning about cyberinfrastructure and related concepts using a web-based platform that will take approximately one hour per lesson. Participation in this study is voluntary.\n",
    "\n",
    "Participants in this research must be 18 years or older. If you are under the age of 18 then please exit this webpage or navigate to another website such as the Hour of Code at https://hourofcode.com, which is designed for K-12 students.\n",
    "\n",
    "If you are not interested in participating please exit the browser or navigate to this website: http://www.umn.edu. Your participation is voluntary and you are free to stop the lesson at any time.\n",
    "\n",
    "For the full description please navigate to this website: <a href=\"../../gateway-lesson/gateway/gateway-1.ipynb\">Gateway Lesson Research Study Permission</a>.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide"
    ]
   },
   "outputs": [],
   "source": [
    "# This code cell starts the necessary setup for Hour of CI lesson notebooks.\n",
    "# First, it enables users to hide and unhide code by producing a 'Toggle raw code' button below.\n",
    "# Second, it imports the hourofci package, which is necessary for lessons and interactive Jupyter Widgets.\n",
    "# Third, it helps hide/control other aspects of Jupyter Notebooks to improve the user experience\n",
    "# This is an initialization cell\n",
    "# It is not displayed because the Slide Type is 'Skip'\n",
    "\n",
    "from IPython.display import HTML, IFrame, Javascript, display\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "\n",
    "import getpass # This library allows us to get the username (User agent string)\n",
    "\n",
    "# import package for hourofci project\n",
    "import sys\n",
    "sys.path.append('../../supplementary') # relative path (may change depending on the location of the lesson notebook)\n",
    "import hourofci\n",
    "\n",
    "# load javascript to initialize/hide cells, get user agent string, and hide output indicator\n",
    "# hide code by introducing a toggle button \"Toggle raw code\"\n",
    "HTML(''' \n",
    "    <script type=\"text/javascript\" src=\\\"../../supplementary/js/custom.js\\\"></script>\n",
    "    \n",
    "    <style>\n",
    "        .output_prompt{opacity:0;}\n",
    "    </style>\n",
    "    \n",
    "    <input id=\"toggle_code\" type=\"button\" value=\"Toggle raw code\">\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Distributed Spatial Computing?\n",
    "\n",
    "<center><img src=https://media.makeameme.org/created/why-even-bother-5c8eb2.jpg width=300></center>\n",
    "In recent years, the spatial technology has evolved tremendously resulted in BIG spatial data. Some examples of spatial big data include location-based services like Uber, Lyft, scooter ride companies and many more, remote sensing data, spatial social networks' data like twitter and FaceBook, weather maps, transportation, and countless others. Handling such BIG load of spatial data needs <b>faster</b> database management technologies, and parallel computing is faster!\n",
    "\n",
    "By the way, if you are curious about spatial big data, we talked about it in the <a href=\"http://try.hourofci.org/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fhourofci%2Flessons&urlpath=tree%2Flessons%2Fintermediate-lessons%2Fbig-data%2FWelcome.ipynb&branch=master\">intermediate Big Data</a> lesson. So, take a look if you have not already.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Sedona\n",
    "\n",
    "SO far, we briefly saw how Apache Spark works but like most data management technologies, Apache Spark also first was developed for non-spatial data. Why? well, because <b>spatial is special!</b> Due to nature of spatial data type, it is much more complex and therefore harder to store and analyse. \n",
    "To support spatial data type, people at Apache launched an extension to Spark named <b><a href=\"http://sedona.apache.org\">Apache Sedona</a></b>. \n",
    "\n",
    "Apache Sedona (formerly GeoSpark) is a powerful tool that extends RDDs to geospatial RDDs (aka SpatialRDD). In simple words Apache Sedona enables two major things: \n",
    "<ol>\n",
    "    <li>\n",
    "Distributing geospatial data between multiple computational cores \n",
    "    </li>\n",
    "    <li>\n",
    "Spatial functions and queries in SparkSQL\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "In this segment we touch on Apache Sedona and see how it works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import `SparkSession` from SparkSQL as the initial step in Spark framework. Then we need to import a few sub-modules from Sedona. But we need to install the `Apache Sedona` package first. Let's do it in the next cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install apache-sedona --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, click the \"Restart Kernel\" to update the list of installed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "def restarter():\n",
    "    display(HTML(\n",
    "        '''\n",
    "            <script>\n",
    "                code_show = false;\n",
    "                function restart_kernel(){\n",
    "                    IPython.notebook.kernel.restart();\n",
    "                }\n",
    "            </script>\n",
    "            <button onclick=\"restart_kernel()\">Restart Kernel</button>\n",
    "        '''\n",
    "    ))\n",
    "restarter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import a couple of general packages for visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from ipyleaflet import Map, GeoData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a spatially enabled spark context using the imported Sedona sub-modules. Don't worry too much if it looks compicated! You can copy and paste this for your project :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"Spatial Spark Demo\").\\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) .\\\n",
    "    config(\"spark.jars.packages\", \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.2.1-incubating,org.datasyslab:geotools-wrapper:1.1.0-25.2\") .\\\n",
    "    getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading demo shapefiles to illustrate Apache Sedona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this segment, we will use two shapefiles to demonstrate the functionalities of Apache Sedona. These shapefiles include a rivers' and the US state boundaries dataset.\n",
    "\n",
    "Ok, let's download the dataset of rivers and lake centerlines for the entire world using `wget`. This is the same dataset we used <a href=\"http://try.hourofci.org/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fhourofci%2Flessons&urlpath=tree%2Flessons%2Fbeginner-lessons%2Fgeospatial-data%2Fgd-example_1.ipynb&branch=master\">here</a> in the beginner geospatial data lesson. You can learn more about this dataset there. \n",
    "\n",
    "Run the cell below to download the dataset as a zip file and then extract the shapefile using `unzip`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O ne_10m_rivers_lake_centerlines.zip https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/physical/ne_10m_rivers_lake_centerlines.zip \n",
    "!unzip -n ne_10m_rivers_lake_centerlines.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the shapefile using geopandas and take a look at the first few rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers = gpd.read_file('ne_10m_rivers_lake_centerlines.shp')\n",
    "rivers = rivers[['featurecla', 'name', 'name_alt', 'rivernum', 'geometry']]\n",
    "rivers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, there is a `geometry` column there too! That's what makes it a spatial dataset. Now let's plot it and see the line features of rivers and lake centerlines on the map. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers_layer = GeoData(geo_dataframe = rivers, style={'color':'blue'})\n",
    "mymap1 = Map(center=(40,10), zoom = 2)\n",
    "mymap1.add_layer(rivers_layer)\n",
    "mymap1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! \n",
    "\n",
    "Now we will do the same for the `US states` shapefile. This is a shapefile of the US states' boundaries downloaded from the <a href=\"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjBpayMwJn7AhV2lGoFHbfWB_cQFnoECAsQAQ&url=https%3A%2F%2Fwww.census.gov%2Fgeographies%2Fmapping-files%2Ftime-series%2Fgeo%2Fcarto-boundary-file.html&usg=AOvVaw2QKo7f-rChpkoO7zQ9E75A\">US Census government</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O us_states.zip https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_20m.zip \n",
    "!unzip -n us_states.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = gpd.read_file('us_states.zip')\n",
    "states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "states_layer = GeoData(geo_dataframe = states, style={'color':'red'})\n",
    "mymap2 = Map(center=(40,-100), zoom = 4)\n",
    "mymap2.add_layer(states_layer)\n",
    "mymap2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting GeoPandas to Apache Sedona\n",
    "\n",
    "So far, the shapefiles have been loaded as geopandas dataframes. In order to enable parallel computation, we need to convert them to spark dataframes. This is easy! Just use Spark's `createDataFrame` method as below. \n",
    "\n",
    "Here we also print the dataframe's schema to see what columns and data types we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_spdf = spark.createDataFrame(states)\n",
    "states_spdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that at the very buttom of the schema there is a `geometry` data type! That's what Sedona braught to us. \n",
    "\n",
    "Next we use `show` method to see a few first rows of the spark dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "states_spdf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And same process for the rivers' dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers_spdf = spark.createDataFrame(rivers)\n",
    "rivers_spdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers_spdf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating SQL Views (Virtual Tables)\n",
    "\n",
    "Similar to non-spatial data, we need to create SQL Views for each of the dataframes we have. This is a required step to enable querying data in SQL and literally means creating virtual relations (tables) for dataframes. Below, we do this using `createOrReplaceTempView` method and create two Views named `rivers` and `states`. We will query from these two tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers_spdf.createOrReplaceTempView(\"rivers\")\n",
    "states_spdf.createOrReplaceTempView(\"states\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Demo Spatial Query \n",
    "As an example, suppose we want to see which rivers and lake centerlines cross the boundaries of Minnesota and Washington states. This is indeed a spatial query as we care about the geographical (topological) relations of features. \n",
    "\n",
    "One way to write this query is as follows:<br>\n",
    "In this SQL query, we select state and river names along with the rivers' geometry. \n",
    "\n",
    "Under the `WHERE` clause, we say if the state name is either Minnesota OR Washington AND the rivers that intersect the polygons of these two states. \n",
    "\n",
    "```sql\n",
    "SELECT s.NAME, r.name river_name, r.geometry geom\n",
    "FROM states s, rivers r\n",
    "WHERE s.NAME IN ('Minnesota', 'Washington') and ST_INTERSECTS(r.geometry, s.geometry)\n",
    "```\n",
    "\n",
    "Let's execute this spatial query together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_rivers = spark.sql(\"\"\"\n",
    "SELECT s.NAME, r.name river_name, r.geometry geom\n",
    "FROM states s, rivers r\n",
    "WHERE s.NAME IN ('Minnesota', 'Washington') and ST_INTERSECTS(r.geometry, s.geometry)\n",
    "\"\"\")\n",
    "\n",
    "mn_rivers.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Friends, spatial queries like this one would take much longer if you do not partition them. Please note that this difference is more significant for larger datasets as the \"distribution\" of data between multiple cores itself could be time consuming. Hence, for small datasets we do not usually use parallel computing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Apache Sedona to GeoPandas\n",
    "\n",
    "Ok, we queried our data and returned all rivers that have the conditions we set. Now to spatially visualize them we need to convert the result back to geopandas dataframe. To do this we simply use `toPandas` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_rivers_df = mn_rivers.toPandas()\n",
    "result = gpd.GeoDataFrame(mn_rivers_df, geometry=\"geom\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the result on a map! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_mn = states[(states['NAME']=='Minnesota') | (states['NAME']=='Washington')]\n",
    "wa_mn_layer = GeoData(geo_dataframe = wa_mn, style={'color':'red'})\n",
    "gdf_layer = GeoData(geo_dataframe = result, style={'color':'blue'})\n",
    "gdf_map = Map(center=(40,-100), zoom = 4)\n",
    "gdf_map.add_layer(gdf_layer)\n",
    "gdf_map.add_layer(wa_mn_layer)\n",
    "gdf_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now, click the link below to go to the exploration segment to dig in even more! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\"><a style=\"background-color:blue;color:white;padding:12px;margin:10px;font-weight:bold;\" \n",
    "href=\"pc-exploration.ipynb\">Click here to go to the next notebook.</a></font>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
