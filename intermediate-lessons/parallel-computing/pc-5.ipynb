{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intermediate Parallel Computing\n",
    "\n",
    "### Part 4 of 5\n",
    "\n",
    "### PySpark SQL II: Non-Spatial SQL Query\n",
    "\n",
    "### In this segment we will learn:\n",
    "* Spark SQL and spark dataframes.\n",
    "* Querying non-spatial data with PySpark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder\n",
    "<a href=\"#/slide-2-0\" class=\"navigate-right\" style=\"background-color:blue;color:white;padding:8px;margin:2px;font-weight:bold;\">Continue with the lesson</a>\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "<font size=\"+1\">\n",
    "\n",
    "By continuing with this lesson you are granting your permission to take part in this research study for the Hour of Cyberinfrastructure: Developing Cyber Literacy for GIScience project. In this study, you will be learning about cyberinfrastructure and related concepts using a web-based platform that will take approximately one hour per lesson. Participation in this study is voluntary.\n",
    "\n",
    "Participants in this research must be 18 years or older. If you are under the age of 18 then please exit this webpage or navigate to another website such as the Hour of Code at https://hourofcode.com, which is designed for K-12 students.\n",
    "\n",
    "If you are not interested in participating please exit the browser or navigate to this website: http://www.umn.edu. Your participation is voluntary and you are free to stop the lesson at any time.\n",
    "\n",
    "For the full description please navigate to this website: <a href=\"../../gateway-lesson/gateway/gateway-1.ipynb\">Gateway Lesson Research Study Permission</a>.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide"
    ]
   },
   "outputs": [],
   "source": [
    "# This code cell starts the necessary setup for Hour of CI lesson notebooks.\n",
    "# First, it enables users to hide and unhide code by producing a 'Toggle raw code' button below.\n",
    "# Second, it imports the hourofci package, which is necessary for lessons and interactive Jupyter Widgets.\n",
    "# Third, it helps hide/control other aspects of Jupyter Notebooks to improve the user experience\n",
    "# This is an initialization cell\n",
    "# It is not displayed because the Slide Type is 'Skip'\n",
    "\n",
    "from IPython.display import HTML, IFrame, Javascript, display\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "\n",
    "import getpass # This library allows us to get the username (User agent string)\n",
    "\n",
    "# import package for hourofci project\n",
    "import sys\n",
    "sys.path.append('../../supplementary') # relative path (may change depending on the location of the lesson notebook)\n",
    "import hourofci\n",
    "\n",
    "# load javascript to initialize/hide cells, get user agent string, and hide output indicator\n",
    "# hide code by introducing a toggle button \"Toggle raw code\"\n",
    "HTML(''' \n",
    "    <script type=\"text/javascript\" src=\\\"../../supplementary/js/custom.js\\\"></script>\n",
    "    \n",
    "    <style>\n",
    "        .output_prompt{opacity:0;}\n",
    "    </style>\n",
    "    \n",
    "    <input id=\"toggle_code\" type=\"button\" value=\"Toggle raw code\">\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().setAppName(\"hourofci\").setMaster(\"local[*]\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrames\n",
    "\n",
    "Spark dataframes are organized data in rows and columns that are distributed between multiple computational cores. In other words, Spark DataFrames are very similar to Pandas dataframes except they are distributed. Spark dataframes are faster and more convenient to use compared to RDDs. \n",
    "\n",
    "In the next cell, we use `read` method followed by `toDF` method to load our favorite film's csv file as a Spark dataframe. Using the `option` method we indicate that our csv file has a header that should be skipped. Please note that we set column names for our new dataframe. The column names can be different than the header of the csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(\"supplementary/films.csv\").\\\n",
    "toDF('index','Title','Year','Length','Subject','Popularity')\n",
    "films.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Our Life-Finder Query!\n",
    "\n",
    "In the previous segment we used `filter` method to retrieve all movies that have the word \"Life\" in their title from the film's RDD. Here, we want to write and execute actual SQL query to return the same result. \n",
    "\n",
    "But remember from the <a href=\"http://try.hourofci.org/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fhourofci%2Flessons&urlpath=tree%2Flessons%2Fintermediate-lessons%2Fgeospatial-data%2FWelcome.ipynb&branch=master\">intermediate lesson on Geospatial Data</a> to be able to execute SQL codes, we need relations (i.e., tables or views) that are different from dataframes. \n",
    "\n",
    "Spark SQL provides an environment to create SQL *Views* (i.e., virtual tables) from spark dataframes. We can do this using `createOrReplaceTempView` as follows. We name our View as `films_table`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films.createOrReplaceTempView(\"films_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, time to write and execute a SQL query to fetch all movies containing the word \"Life\" in their title. The following SQL query is one way to do this. \n",
    "\n",
    "\n",
    ">```sql\n",
    "SELECT *\n",
    "FROM films_table f\n",
    "WHERE f.Title LIKE '%Life%'\n",
    "```\n",
    "\n",
    "In this query, we select all (`*`) rows from the films_table that is renamed to `f` that meet our condition in the `WHERE`-clause. The condition is that the title includes (`LIKE` function) the word \"Life\". The percentage (`%`) symbol means there could be any character comming before and after the word of interest. \n",
    "\n",
    "In the cell below, we execute this query using `sql` method and print the result using the `show()` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_result = spark.sql(\n",
    "            \"\"\"\n",
    "            SELECT *\n",
    "            FROM films_table f\n",
    "            WHERE f.Title LIKE '%Life%'\n",
    "            \"\"\")\n",
    "\n",
    "film_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One More Example \n",
    "\n",
    "Cool! To practice this more, let's look at the next query. \n",
    "><b>Query:</b> Select all films that are shorter than 30 minutes and are highly popular (i.e., have popularity index higher than 50).\n",
    "\n",
    "<b>Solution:</b> \n",
    "And here is the SQL query that can address the query. \n",
    ">```sql\n",
    "SELECT *\n",
    "FROM films_table f\n",
    "WHERE f.length < 30 and f.popularity > 50```\n",
    "\n",
    "Let's execute it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_films = spark.sql(\n",
    "            \"\"\"\n",
    "            SELECT *\n",
    "            FROM films_table f\n",
    "            WHERE f.length < 30 and f.popularity > 50\n",
    "            \"\"\")\n",
    "\n",
    "short_films.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Spark DataFrames to Pandas \n",
    "\n",
    "Once the query is executed using multiple cores, sometimes we want to convert the results to regular Pandas dataframes to use Pandas's functionalities. We can do this as simple as using `toPandas()` method as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_result_df = film_result.toPandas()\n",
    "film_result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Pandas DataFrame to Spark DataFrame\n",
    "\n",
    "You might wonder what if we have a Pandas dataframe and we want to convert it to Spark DataFrame and try parallel computing?\n",
    "\n",
    "Well, Spark's `createDataFrame` function will do it. In the cell below we convert our `film_result_df` dataframe back to Spark dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(film_result_df).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What About Spatial Datasets?\n",
    "So far so good, but what if we want to load a shapefile as a Spark dataframe? \n",
    "\n",
    "A very first thought would be loading the shapefile into a GeoDataFrame and then converting it to Spark dataframe, right? \n",
    "\n",
    "Let's try it then!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "gdf = gpd.read_file(\"supplementary/MN_points/POINT.shp\")\n",
    "try:\n",
    "    points = spark.createDataFrame(gdf)\n",
    "except ValueError as err:\n",
    "    print(\"ValueError: \",err)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops!!! There is a ValueError! Seems like Spark is complaining about the `geometry` column and its data type.\n",
    "\n",
    ">Why do you think this error occurred? <br>\n",
    "What solution do you propose to debug?\n",
    "\n",
    "Let us know in the textbox below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "w = widgets.Textarea(\n",
    "            value='',\n",
    "            placeholder='Write your answers here',\n",
    "            description='',\n",
    "            disabled=False,\n",
    "            layout=Layout( height='200px', min_height='100px', width='900px')\n",
    "            )\n",
    "\n",
    "def out1():\n",
    "    print('Submitted! See one debugging approach below.')\n",
    "    \n",
    "display(w)\n",
    "hourofci.SubmitBtn2(w, out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to debug this code is to get rid of the `geometry` data type. To do this, we can convert the type of `geometry` column to `string` type using `astype('string')` method and then try converting it to Spark dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['geometry'] = gdf['geometry'].astype('string')\n",
    "\n",
    "points = spark.createDataFrame(gdf)\n",
    "points.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, seems like we could kinda debug our code, but the problem is that we no longer have the geometry data type. The `geometry` column simply contains some useless string values. \n",
    "\n",
    "This is because PySpark **does not** support spatial data. Therefore, we need to install and use the spatial extension of Spark. \n",
    "\n",
    "In the next segment, we will see how to handle spatial data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font size=\"+1\"><a style=\"background-color:blue;color:white;padding:12px;margin:10px;font-weight:bold;\" \n",
    "href=\"pc-6.ipynb\">Click here to go to the next notebook.</a></font>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "rise": {
   "autolaunch": false,
   "overlay": "<div class='hciheader'></div><div class='hcifooter'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
