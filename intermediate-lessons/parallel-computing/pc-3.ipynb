{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to Parallel Computing\n",
    "### Part 2 of 5\n",
    "\n",
    "### Apache Spark, the Spark of a Journey!\n",
    "\n",
    "### In this segment we will answer:\n",
    "* What is Apache Spark?\n",
    "* What is it's architecture?\n",
    "* How to install and initiate a Spark computing session?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thank you for helping our study\n",
    "\n",
    "\n",
    "<a href=\"#/slide-1-0\" class=\"navigate-right\" style=\"background-color:blue;color:white;padding:8px;margin:2px;font-weight:bold;\">Continue with the lesson</a>\n",
    "\n",
    "Throughout this lesson you will see reminders, like the one below, to ensure that all participants understand that they are in a voluntary research study.\n",
    "\n",
    "### Reminder\n",
    "\n",
    "<font size=\"+1\">\n",
    "\n",
    "By continuing with this lesson you are granting your permission to take part in this research study for the Hour of Cyberinfrastructure: Developing Cyber Literacy for GIScience project. In this study, you will be learning about cyberinfrastructure and related concepts using a web-based platform that will take approximately one hour per lesson. Participation in this study is voluntary.\n",
    "\n",
    "Participants in this research must be 18 years or older. If you are under the age of 18 then please exit this webpage or navigate to another website such as the Hour of Code at https://hourofcode.com, which is designed for K-12 students.\n",
    "\n",
    "If you are not interested in participating please exit the browser or navigate to this website: http://www.umn.edu. Your participation is voluntary and you are free to stop the lesson at any time.\n",
    "\n",
    "For the full description please navigate to this website: <a href=\"../../gateway-lesson/gateway/gateway-1.ipynb\">Gateway Lesson Research Study Permission</a>.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "Hide"
    ]
   },
   "outputs": [],
   "source": [
    "# This code cell starts the necessary setup for Hour of CI lesson notebooks.\n",
    "# First, it enables users to hide and unhide code by producing a 'Toggle raw code' button below.\n",
    "# Second, it imports the hourofci package, which is necessary for lessons and interactive Jupyter Widgets.\n",
    "# Third, it helps hide/control other aspects of Jupyter Notebooks to improve the user experience\n",
    "# This is an initialization cell\n",
    "# It is not displayed because the Slide Type is 'Skip'\n",
    "\n",
    "from IPython.display import HTML, IFrame, Javascript, display\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "\n",
    "import getpass # This library allows us to get the username (User agent string)\n",
    "\n",
    "# import package for hourofci project\n",
    "import sys\n",
    "sys.path.append('../../supplementary') # relative path (may change depending on the location of the lesson notebook)\n",
    "import hourofci\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Hide warnings\n",
    "\n",
    "# load javascript to initialize/hide cells, get user agent string, and hide output indicator\n",
    "# hide code by introducing a toggle button \"Toggle raw code\"\n",
    "# HTML(''' \n",
    "#     <script type=\"text/javascript\" src=\\\"../../supplementary/js/custom.js\\\"></script>\n",
    "    \n",
    "#     <input id=\"toggle_code\" type=\"button\" value=\"Toggle raw code\">\n",
    "# ''')\n",
    "\n",
    "HTML(''' \n",
    "    <script type=\"text/javascript\" src=\\\"../../supplementary/js/custom.js\\\"></script>\n",
    "    \n",
    "    <style>\n",
    "        .output_prompt{opacity:0;}\n",
    "    </style>\n",
    "    \n",
    "    <input id=\"toggle_code\" type=\"button\" value=\"Toggle raw code\">\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Apache Spark?\n",
    "\n",
    "Apache Spark is an open-source data processing framework designed to carry out processing tasks on large volumes of data. It enables dividing of data into smaller chuncks and distributing them between multiple processing units to perform data processing tasks at the same time. \n",
    "\n",
    "\n",
    "\n",
    "<!-- <img src=\"https://www.interviewbit.com/blog/wp-content/uploads/2022/06/Apache-Spark-1536x714.png?raw=1\" width=\"80%\"> -->\n",
    "<!-- <img src=\"https://blog.kakaocdn.net/dn/uO5KB/btqA2a0rio3/n95MXm1PSKxJzz1oaoCA21/img.png?raw=1\" width=\"50%\"> -->\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Spark architecture\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: #fff; text-align: left; vertical-align:\">\n",
    "        <td style=\"background: #fff; text-align: left; font-size: 23px;\">\n",
    "            <ul>\n",
    "                <li>\n",
    "            The Driver Program first executes and builds a SparkSession along with a few other components such as a Backend Scheduler, Task Scheduler, DAG Scheduler, and Block Manager that are all responsible to translate your code into tasks. Then a request for resource allocation is sent to cluster manager. The recources include worker nodes, executers, and memory allocations. Allocating these resources to jobs, instructing the workers, tracking the jobs, and reporting back the job status are all among the Cluster Manager responsibilities. \n",
    "                </li>\n",
    "                <li>\n",
    "            Please note that the number of workers and the amount of other resources are set by the user in the SparkContex. According to the user's configuration, SparkContext, that is a sub-class of SparkSession, breaks down the job into the smaller pieces called taks.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </td>\n",
    "        <td style=\"width: 50%; background: #fff; text-align: left; vertical-align: top;\"> \n",
    "            <img src=\"supplementary/spark.png\" width=\"100%\">\n",
    "            <a href=https://data-flair.training/blogs/how-apache-spark-works/>Image source</a>\n",
    "            <li style=\"background: #fff; text-align: left; font-size: 23px;\">\n",
    "            Finally, the cluster manager launches the tasks through multiple Worker Nodes. Each worker node can have different number of Executers that execute tasks in parallel. \n",
    "            </li>\n",
    "    </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Pretty confusing hah?! Don't worry, you'll get a better sense as we go through it in action later in this segment! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before jumping into the code...\n",
    "<table>\n",
    "    <tr style=\"background: #fff; text-align: left; vertical-align:\">\n",
    "        <td style=\"background: #fff; text-align: left; font-size: 23px;\">\n",
    "            <p>Apache Spark has a core engine and four extensions:</p>\n",
    "            <ul>\n",
    "                <li>\n",
    "<b>Spark SQL (enables executing SQL queries on data)</b>\n",
    "                </li>\n",
    "                <li>\n",
    "Spark Streaming (streaming data add-on)\n",
    "                </li>\n",
    "                <li>\n",
    "Spark MLlib (set of machine learning libraries)\n",
    "                </li>\n",
    "                <li>\n",
    "GraphX (designed for distributed graph processing Ex. LinkedIn, Facebook, etc).\n",
    "                </li>\n",
    "            </ul>\n",
    "</td>\n",
    "     <td style=\"width: 40%; background: #fff; text-align: left; vertical-align: top;\"> <img src='https://blog.kakaocdn.net/dn/uO5KB/btqA2a0rio3/n95MXm1PSKxJzz1oaoCA21/img.png?raw=1' width=\"500\" height=\"700\" alt='map'></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "In this lesson, we will focus on **Spark SQL** only! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark in Memory\n",
    "\n",
    "Spark works in the **in-memory** computing paradigm. It means that the data processing happens in Random Access Memory (RAM) that results in significant performance gains. \n",
    "\n",
    "Spark enables both **batch processing** and **stream processing**. \n",
    "\n",
    "**\\*** In **Batch processing** batches (chunks) of data are usually stored on the local machine (on Hard Disc Drive) and each batch gets processed at once.<br>\n",
    "**\\*** In **Stream processing** data that are stored in a remote place flow into the system and the stream of the data gets processed in real-time.\n",
    "\n",
    "Ok, enough intro, let's dig in! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, we need to install the Python API of Spark. It's called PySpark. \n",
    "\n",
    "Once the installation finished, click the \"Restart Kernel\" to update packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def restarter():\n",
    "    display(HTML(\n",
    "        '''\n",
    "            <script>\n",
    "                code_show = false;\n",
    "                function restart_kernel(){\n",
    "                    IPython.notebook.kernel.restart();\n",
    "                }\n",
    "            </script>\n",
    "            <button onclick=\"restart_kernel()\">Restart Kernel</button>\n",
    "        '''\n",
    "    ))\n",
    "restarter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Allright, now we build a Spark Session. \n",
    "\n",
    "We use SparkConf() function to set a name to our application and specify the number of threads for our program to run. \n",
    "\n",
    "Here, we call it 'hourofci' and specify 4 **threads** in our local machine, then store the session in a variable named `spark`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().setAppName(\"hourofci\").setMaster(\"local[4]\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cool! Now we have a platform to perform our data analysis using 4 threads in parallel! \n",
    "\n",
    "Now, quiz time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "Hide",
     "Init"
    ]
   },
   "outputs": [],
   "source": [
    "widget1 = widgets.RadioButtons(\n",
    "    options = ['No', 'Yes'],\n",
    "    description = 'Is the number of threads equivalent to the number of physical CPU cores in our machine?', style={'description_width': 'initial'},\n",
    "    layout = Layout(width='100%',display=\"flex\", justify_content=\"flex-start\"),\n",
    "    value = None\n",
    ")\n",
    "\n",
    "display(widget1)\n",
    "def out():\n",
    "    return print('They are not the same, actually!')\n",
    "\n",
    "hourofci.SubmitBtn2(widget1, out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallelising Using Threads!\n",
    "\n",
    "The number of threads is the number of gates you specify to process your data in parallel. It is usually specified as the number of available physical cores. We saw how to used `setMaster()` function to pass the variable `local` to specify the number of threads. The followings are the available parallelism options:\n",
    "\n",
    "* `local`: Run Spark with a single worker thread - No parallelism at all!\n",
    "\n",
    "* `local[N]`: Run Spark with N multiple worker threads.\n",
    "\n",
    "* `local[*]`: Set the number of worker threads as same as the number of available physical CPU cores on your machine.\n",
    "\n",
    "Please note that we have two other oprions as `local[N,F]` and `local[*,F]` for specifying F maxFailures that we will not get into it's details in this lesson. See <a href=\"https://spark.apache.org/docs/latest/configuration.html\">here</a> for more information. \n",
    "\n",
    "If you are curious to see how many physical cores you have right now, run the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print('The number of available physical CPU cores: ', os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You know have a gentle idea that what Spark is and how to initiate one!<br> \n",
    "In the next segment we will get into some more details of **Spark SQL**.<br>\n",
    "\n",
    "\n",
    "\n",
    "<font size=\"+1\"><a style=\"background-color:blue;color:white;padding:12px;margin:10px;font-weight:bold;\" href=\"pc-4.ipynb\">Click here to go to the next notebook.</a></font>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "rise": {
   "autolaunch": true,
   "overlay": "<div class='hciheader'></div><div class='hcifooter'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
